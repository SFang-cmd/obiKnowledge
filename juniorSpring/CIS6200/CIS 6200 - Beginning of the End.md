Not about statistical learning:
- Paper-reading + presentations
- Heavy use of LLMs, give or don't give API credits

What is this course?
- You will learn the basics and milestones of how we got from a pre-ChatGPT world to where we are now
- Learn basics of modern image generation models work: Diffusion models and flow matching
- Use 1 and read 2 cutting edge, 2025 research in these areas
- Learn how these ideas translate to applications in the physical sciences
	- Language models --> Protein/DNA/RNA language models
	- Image generation --> Protein and molecule generation
- Build a system that uses what you've learned and does something interesting

Course different from previous sems
- More focused on the cutting edge AI models nowadays

Topics:
- Tokenization, word embeddings, older models in NLP
- Attention, transformers, autoregressive decoding
- Alignment I: SFT RLHF/PPO, DPO
- Alignment II: Chain of thought, RLVF, GRPO, ORMs
- Tool usage and interleaved reasoning
- Retrieval, retrieval augmented generation
- Putting it all together: Pydantic AI
- Variational inference
- Variational autoencoders
- Diffusion models
- Flow matching
- Multimodel modeling
- Applications in science
- Rest of the class is **paper presentations** (literally 2nd half for the entire part)

Applied tool use is very important

